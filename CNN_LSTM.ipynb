{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load benign image data\n",
    "\n",
    "import pickle\n",
    "with open('Cleaned_Breast_Types/benign_list.pkl', 'rb') as file:\n",
    "    loaded_img_list = pickle.load(file)\n",
    "    \n",
    "normal_list=[]\n",
    "x = 0\n",
    "while True:\n",
    "  try:\n",
    "    array_a_loaded = loaded_img_list[x]\n",
    "    normal_list.append(array_a_loaded)\n",
    "    x+=1\n",
    "  except:\n",
    "    print('end!')\n",
    "    break\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cancer image data\n",
    "\n",
    "import pickle\n",
    "with open('Cleaned_Breast_Types/cancer_list.pkl', 'rb') as file:\n",
    "    loaded_img_list = pickle.load(file)\n",
    "    \n",
    "cancer_list=[]\n",
    "x = 0\n",
    "while True:\n",
    "  try:\n",
    "    array_a_loaded = loaded_img_list[x]\n",
    "    cancer_list.append(array_a_loaded)\n",
    "    x+=1\n",
    "  except:\n",
    "    print('end!')\n",
    "    break\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normal image data\n",
    "\n",
    "import pickle\n",
    "with open('Cleaned_Breast_Types/normal_list.pkl', 'rb') as file:\n",
    "    loaded_img_list = pickle.load(file)\n",
    "    \n",
    "cancer_list=[]\n",
    "x = 0\n",
    "while True:\n",
    "  try:\n",
    "    array_a_loaded = loaded_img_list[x]\n",
    "    cancer_list.append(array_a_loaded)\n",
    "    x+=1\n",
    "  except:\n",
    "    print('end!')\n",
    "    break\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset (Normal and Cancer only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine the lists and create labels (normal and cancer)\n",
    "X = np.array(normal_list + cancer_list)\n",
    "y = np.array([0] * len(normal_list) + [1] * len(cancer_list))\n",
    "\n",
    "# train 0.8, test 0.1, val 0.1\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=True,random_state=42,stratify=y)\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test,y_test,test_size=0.5,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - LSTM (1st Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, TimeDistributed, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Adjust the input shape to include the number of channels (e.g., grayscale images have 1 channel)\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    " \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    " \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    " \n",
    "model1 = build_cnn_lstm_model(input_shape)\n",
    "model1.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "# Convert your data to tf.data.Dataset for better performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    "# Train the model\n",
    "history1 = model1.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=valid_dataset\n",
    "    # callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the valid set\n",
    "test_loss, test_acc = model1.evaluate(valid_dataset)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and Classification Report (1st Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Predict probabilities for the validation set\n",
    "predictions = model1.predict(valid_dataset)\n",
    "predicted_labels = (predictions > 0.5).astype(\"int32\")  # Apply a threshold to get binary labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, predicted_labels)\n",
    "print(f'Validation accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(y_valid, predicted_labels, digits=5))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_valid, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve (1st Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    " \n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - LSTM (2nd Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, TimeDistributed, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Adjust the input shape to include the number of channels (e.g., grayscale images have 1 channel)\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    " \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    " \n",
    "model2 = build_cnn_lstm_model(input_shape)\n",
    "model2.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "# Convert your data to tf.data.Dataset for better performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    "# Train the model\n",
    "history2 = model2.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=valid_dataset\n",
    "    # callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the valid set\n",
    "test_loss, test_acc = model2.evaluate(valid_dataset)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and Classification Report (2nd Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Predict probabilities for the validation set\n",
    "predictions = model2.predict(valid_dataset)\n",
    "predicted_labels = (predictions > 0.5).astype(\"int32\")  # Apply a threshold to get binary labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, predicted_labels)\n",
    "print(f'Validation accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(y_valid, predicted_labels, digits=5))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_valid, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve (2nd Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    " \n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - LSTM (3rd Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, TimeDistributed, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Adjust the input shape to include the number of channels (e.g., grayscale images have 1 channel)\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    " \n",
    "model3 = build_cnn_lstm_model(input_shape)\n",
    "model3.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "# Convert your data to tf.data.Dataset for better performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    "# Train the model\n",
    "history3 = model3.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=valid_dataset\n",
    "    # callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the valid set\n",
    "test_loss, test_acc = model3.evaluate(valid_dataset)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and Classification Report (3rd Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Predict probabilities for the validation set\n",
    "predictions = model3.predict(valid_dataset)\n",
    "predicted_labels = (predictions > 0.5).astype(\"int32\")  # Apply a threshold to get binary labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, predicted_labels)\n",
    "print(f'Validation accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(y_valid, predicted_labels, digits=5))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_valid, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve (3rd Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history3.history['accuracy'])\n",
    "plt.plot(history3.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    " \n",
    "# summarize history for loss\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Tuning (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, TimeDistributed, Flatten, LSTM, Dropout, Dense\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperParameters\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def build_cnn_lstm_model(hp):\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(Conv2D(filters=hp.Int('filters1', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu', input_shape=(256, 256, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=hp.Int('filters2', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=hp.Int('filters3', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=hp.Int('filters4', min_value=32, max_value=128, step=32),\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(units=hp.Int('unit', min_value=32, max_value=128, step=32), return_sequences=False))\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "   \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "   \n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter space for epochs and batch size\n",
    "hp = HyperParameters()\n",
    "hp.Int('epochs', min_value=10, max_value=50, step=10)  # Range of epochs\n",
    "hp.Int('batch_size', min_value=16, max_value=64, step=16)  # Range of batch sizes\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Create the RandomSearch tuner\n",
    "tuner = RandomSearch(\n",
    "    build_cnn_lstm_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='/kaggle/working/',\n",
    "    project_name='cnnlstmtuning',\n",
    "    hyperparameters=hp\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(X_train, y_train, validation_data=(X_valid, y_valid))\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filter 1: \", best_hps.get('filters1'))\n",
    "print(\"Filter 2: \", best_hps.get('filters2'))\n",
    "print(\"Filter 3: \", best_hps.get('filters3'))\n",
    "print(\"Filter 4: \", best_hps.get('filters4'))\n",
    "print(\"Unit: \", best_hps.get('unit'))\n",
    "print(\"Dropout Rate: \", best_hps.get('dropout_rate'))\n",
    "print(\"LR: \", best_hps.get('learning_rate'))\n",
    "print(\"Epochs: \", best_hps.get('epochs'))\n",
    "print(\"BS: \", best_hps.get('batch_size'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild CNN - LSTM Model (2nd Architecture) using Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, TimeDistributed, Flatten, LSTM, Dropout, Dense\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperParameters\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Adjust the input shape to include the number of channels (e.g., grayscale images have 1 channel)\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    " \n",
    "    model.add(Conv2D(96, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    " \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    " \n",
    "model = build_cnn_lstm_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Convert your data to tf.data.Dataset for better performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "\n",
    " \n",
    "# Train the model\n",
    "history1 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=40,\n",
    "    batch_size=32,\n",
    "    validation_data=valid_dataset,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the valid set\n",
    "test_loss, test_acc = model.evaluate(valid_dataset)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix and Classification Report (Tuned Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Predict probabilities for the validation set\n",
    "predictions = model.predict(valid_dataset)\n",
    "predicted_labels = (predictions > 0.5).astype(\"int32\")  # Apply a threshold to get binary labels\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, predicted_labels)\n",
    "print(f'Validation accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(y_valid, predicted_labels, digits=5))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_valid, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curve (Tuned Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    " \n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
